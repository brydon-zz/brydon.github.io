% April 2014
% Brydon Eastman
% Presentation for Linear Algebra

\documentclass{beamer}
\setbeamertemplate{navigation symbols}{}

\usepackage{amsfonts,amssymb,amsmath,tikz}

\usetheme{Warsaw}
\setbeamertemplate{itemize items}[default]
\setbeamertemplate{enumerate items}[default]
\beamersetuncovermixins{\opaqueness<1>{25}}{\opaqueness<2->{15}}
\newcommand{\eastman}{eastman.png}
\newcommand{\oast}{\circledast}
\newcommand{\cC}{{\cal C}}

\def\ow{0.3cm}
\author{Brydon Eastman}

% These are a few colors that I like.

\definecolor{crimsonred}{RGB}{153,0,0}		% Neurtal red, good for dark or light bg
\definecolor{darkcharcoal}{RGB}{25,25,25}		% Darker gray
\definecolor{charcoal}{RGB}{51,51,51}		% Darker gray
\definecolor{ash}{RGB}{100,100,100}			% medium gray
\definecolor{paleblue}{RGB}{0,102,102}		% More of an `ocean' color
\definecolor{turtlegreen}{RGB}{51,153,0}	% A more neutral green
\definecolor{paleale}{RGB}{204,204,102}		% Only for dark BG
\definecolor{lager}{RGB}{140,110,10}		% Use instead of pale ale for white BG
\definecolor{regal}{RGB}{90,0,120}			% A more neutral purple
\definecolor{jeans}{RGB}{20,30,150}			% A more neutral blue


\tikzstyle{small}=[draw,circle,minimum size=0.2mm,inner sep=.5pt,outer sep=-0.1pt,fill=crimsonred]
\tikzset{>=latex,shorten >= 0.1cm,shorten <= 0.1cm}

\beamerdefaultoverlayspecification{<+->}
\title{Image Compression and the Singular Value Decomposition}  
\date{\today} 

\defbeamertemplate*{footline}{shadow theme}
{%
  \leavevmode%
  \hbox{\begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm plus1fil,rightskip=.3cm]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertframenumber\,/\,\inserttotalframenumber\hfill\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle%
  \end{beamercolorbox}}%
  \vskip0pt%
}
\begin{document}
	\begin{frame}
		\begin{center}
			\Huge{\textbf{Image Compression and the Singular Value Decomposition}}
			\vspace{1cm}\\
			\includegraphics[height=1.5cm]{\eastman} \quad
			\includegraphics[height=1.5cm]{Redeemer-Logo.jpg}\\
			\footnotesize\textcolor{gray}{MAT-436 Linear Algebra}\normalsize\\
			\textcolor{normal text.fg!50!}{\textit{Redeemer University College}, \today}\\
			\textcolor{jeans}{\textit{http://linalg.beastman.ca}}\\
			\textcolor{jeans}{\textit{http://cs.redeemer.ca/$\sim$beastman/linalg}}
		\end{center}
	\end{frame}
	
	\begin{frame}{Intro}
		\begin{itemize}
			\item How do computers store images?	\pause
			\begin{center}
				\includegraphics[height=3cm]{deerPixels.jpg}\\
				\onslide<3->{A picture is really a matrix of colours}\\
				\pause\includegraphics[height=4cm]{vegetables.png}
			\end{center}
		\end{itemize}
	\end{frame}
	
	\begin{frame}{The Problem}
		\begin{itemize}
			\item A typical desktop background is $1920$ pixels wide and $1080$ pixels tall
			\item So we'd have to store $1920\times1080$ numbers for the red values, $1920\times1080$ numbers for the green values, and $1920\times 1080$ numbers for the blue values.
			\item That amounts to storing $6,220,800$ different numbers
			\item Computers are good at storing numbers like this, but that's just inefficient
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Why We Need Image Compression}
		\begin{center}
			\includegraphics[height=5cm]{brokenImage.png}
		\end{center}
	
	\end{frame}

	\begin{frame}{The Problem}
		\begin{center}
			\includegraphics[height=5cm]{halfDome.jpg}
		\end{center}
			This is a picture I uploaded to Facebook last summer.\\
			The camera I used stored the $35,426,304$ numbers ($\approx 65 $MB)\\
			On Facebook it was only $12$ MB. What happened?
	\end{frame}
	
	\begin{frame}{The Math}
		\begin{block}{Diagonalisable}A matrix $A$ is diagonalisable if we can find an invertible matrix $P$ such that $A=PDP^{-1}$ where $D$ is a diagonal matrix.\end{block}
		\begin{block}{Symmetric}
			A matrix $A$ is symmetric if $A=A^{\intercal}$. Every symmetric matrix $A$ can be written as $A=PDP^{\intercal}$ where $P$ is an orthogonal matrix and $D$ is a diagonal matrix.
		\end{block}
		\begin{itemize}
			\item In both of these cases $A$ had to be very special
			\item Today we will see that \textit{every} matrix $A$ can be written as $A=PDQ^{\intercal}$. Where $P$ and $Q$ are orthogonal.
			\item This is called \textbf{the singular value decomposition}
		\end{itemize}
	\end{frame}
	
	\begin{frame}{$A^{\intercal}A$}
		\begin{itemize}
			\item Recall that $A^{\intercal}A$ is square, symmetric, and has real eigenvalues (Theorem 5.18, p. 412).
			\item These eigenvalues are all non-negative
		\end{itemize}\pause
		Suppose $\lambda$ is an eigenvalue of $A^{\intercal}A$ with eigenvector $\vec{v}$. Then
		\begin{align*}
			\onslide<5->{0&\le ||A\vec{v}||^2 \\}
			\onslide<7->{&=(A\vec{v})\cdot(A\vec{v})\\}
			\onslide<8->{&=(A\vec{v})^{\intercal}A\vec{v}\\}
			\onslide<9->{&=\vec{v}^{\intercal}A^{\intercal}A\vec{v}\\}
			\onslide<10->{&=\vec{v}^{\intercal}\lambda \vec{v}\\}
			\onslide<11->{&=\lambda ||\vec{v}||^2}
		\end{align*}
	\end{frame}
	
	\begin{frame}{Singular Values}
		\begin{block}{Singular Values}
			If $A$ is an $m\times n$ matrix, the singular values of $A$ are the square roots of the eigenvalues of $A^{\intercal}A$ and are denoted $\sigma_1,\sigma_2,\ldots,\sigma_n$. By convention $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_n$.
		\end{block}
		\begin{align*}
			\onslide<2->{A = \left[\begin{array}{cc}1&1\\1&0\\0&1\end{array}\right]} 	\onslide<4->{&\therefore A^{\intercal} = \left[\begin{array}{ccc}1&1&0\\1&0&1\end{array}\right]\\}\\
			\onslide<5->{A^{\intercal}A &= \left[\begin{array}{cc}2&1\\1&2\end{array}\right]}
		\end{align*}
		\[\onslide<6->{\therefore c(x)=x^2-4x+3}{\hspace{0.2cm}}\onslide<7->\therefore \lambda_1= 3,\lambda_2=1\]
		\[\onslide<9-> \therefore \sigma_1 =\sqrt{3}, \sigma_2 = 1\]
	\end{frame}
	
	\begin{frame}{Some properties of Singular Values}
		\begin{itemize}
			\item If $A$ is an $m\times n$ matrix, then $A^{\intercal}$ is an $n\times m$ matrix.\pause
			\item Thus $A^{\intercal}A$ is an $n\times n$ matrix\pause
			\item An $n\times n$ matrix has a degree $n$ characteristic polynomial\pause
			\item Thus (counting multiplicities) $A^{\intercal}A$ has $n$ eigenvalues.\pause
			\item Therefore $A$ has $n$ singular values (counting multiplicities).\pause
		\end{itemize}
		\pause
		Recall: If $B$ is a $n\times n$ symmetric matrix, then there is an orthonormal basis for $\mathbb{R}^n$ that consists of eigenvectors of $B$. (Proof of the Spectral Theorem, p 414).\pause\\\vspace{0.15cm}
		Let $\{\vec{v}_1,\ldots,\vec{v}_n\}$ be such a basis containing eigenvectors of $A^{\intercal}A$ that correspond with eigenvalues $\lambda_i$.\pause\\\vspace{0.15cm}
		We already saw $||A\vec{v}_i||^2=\lambda_i||\vec{v}_i||^2$. But $\vec{v}_i$ is a unit vector, thus $||A\vec{v}_i||=\sqrt{\lambda_i}=\sigma_i$.
	\end{frame}
	
	\begin{frame}{Singular Value Decomposition}
		\begin{itemize}
			\item We are going to use these singular values to obtain the aforementioned decomposition.\pause
			\item Let $A$ be an $m\times n$ matrix with singuar values $\sigma_1\geq\cdots\geq\sigma_r>0$ and $\sigma_{r+1}=\sigma_{r+2}=\cdots=\sigma_n=0$.
			\item $U$ be an $m\times m$ orthogonal matrix, and $V$ be an $n\times n$ orthogonal matrix.\pause
		\end{itemize}
		\pause
		\[A=U\Sigma V^{\intercal}\]
		\pause
		\begin{center}
			Where $\Sigma=\left[\begin{array}{c|c}D&O\\\hline O&O\end{array}\right]$ and $D=\left[\begin{array}{ccc}\sigma_1&&0\\&\ddots&\\0&&\sigma_r\end{array}\right]$
		\end{center}
	\end{frame}
	
	\begin{frame}{Examples of $\Sigma$}
		Some examples of $\Sigma$ that may show up if $r=2$:
		\[
		\begin{array}{cc}
			\left[\begin{array}{ccc}
			4&0&0\\0&3&0
			\end{array}\right]
			&
			\left[\begin{array}{cc}
			2&0\\0&2\\0&0
			\end{array}\right]\\
			\left[\begin{array}{ccc}
			5&0&0\\0&2&0\\0&0&0\\0&0&0
			\end{array}\right]
			&
			\left[\begin{array}{cc}
			9&0\\0&6
			\end{array}\right]
		\end{array}\]
	\end{frame}
	
	\begin{frame}{How do we do the SVD?}
		\begin{itemize}
			\item We already know how to find $\Sigma$\pause
			\item All we have to do is find $U$ and $V$. We'll find $V$ first.\pause
		\end{itemize}
		\pause
		We know there are eigenvectors $\{\vec{v}_1,\dots,\vec{v}_n\}$ of $A^{\intercal}A$ that form an orthonormal basis for $\mathbb{R}^n$.\\\vspace{0.1cm}\pause
		Then $V=\left[\begin{array}{ccc}\vec{v}_1&\cdots&\vec{v}_n
		\end{array}\right]$ is an orthogonal $n\times n$ matrix.\\\vspace{0.1cm}\pause
		Consider $\{A\vec{v}_1,\ldots,A\vec{v}_n\}$. I claim this is an orthogonal set of vectors. To see this suppose $\vec{v}_j$ is an eigenvector of $A^{\intercal}A$ with corresponding eigenvalue $\lambda_j$. For $i\neq j$ we have.
		\begin{align*}
			\onslide<8->{(A\vec{v}_i)\cdot(A\vec{v}_j) &= (A\vec{v}_i)^{\intercal}(A\vec{v}_j)}\\
			\onslide<10->{&= \vec{v}_i^{\intercal}A^{\intercal}A\vec{v}_j}\\
			\onslide<11->{&= \vec{v}_i^{\intercal}\lambda_j\vec{v}_j }\\
			\onslide<12->{&= \lambda_j(\vec{v}_i\cdot\vec{v}_j)=0}
		\end{align*}
	\end{frame}
	
	\begin{frame}{How do we find $U$ in the SVD?}
		Now recall that $\sigma_i = ||A\vec{v}_i||$ and $r$ of these are nonzero. Thus \\\pause
		\[\left\{\frac{1}{\sigma_1}A\vec{v}_1,\frac{1}{\sigma_2}A\vec{v}_2,\ldots,\frac{1}{\sigma_r}A\vec{v}_r\right\}\] is an orthonormal set in $\mathbb{R}^m$.\vspace{0.1cm}\\\pause
		We call $\vec{u}_i=\frac{1}{\sigma_i}A\vec{v}_i$.\\\pause
		Then let $\vec{u}_{r+1},\ldots,\vec{u}_{m}$ be vectors such that \[U=\left[\begin{array}{cccccc}\vec{u}_1&\cdots&\vec{u}_r&\vec{u}_{r+1}&\cdots&\vec{u}_m\end{array}\right]\]
		is an orthogonal matrix.
	\end{frame}
	
	\begin{frame}{Do these $U$ and $V$ work?}
		Now let's convince ourselves that $A=U\Sigma V^{\intercal}$.\\\vspace{0.15cm}\pause
		Now since $V$ is orthogonal $V^{\intercal}V=I$ (by definition) so it is enough to show
		\[AV=U\Sigma\]\pause
		(Aside: the columns of $V$ are called $\vec{v}_i$ and the columns of $U$ are called $\vec{u}_i$)\\\vspace{0.15cm}\pause
		We constructed $U$ such that $A\vec{v}_i=\sigma_i\vec{u}_i$ for $i=1,\ldots,r$.\\\vspace{0.15cm}\pause
		And $||A\vec{v}_i|| = \sigma_i=0$ for $i=r+1,\ldots,n$.
	\end{frame}
	
	\begin{frame}{Do these $U$ and $V$ work?}
		\begin{align*}
			AV&=A\left[\begin{array}{ccc}\vec{v}_1&\cdots&\vec{v}_n\end{array}\right]\\
			\onslide<3->{&=\left[\begin{array}{ccc}A\vec{v}_1&\cdots&A\vec{v}_n\end{array}\right]}\\
			\onslide<4->{&=\left[\begin{array}{cccccc}\sigma_1\vec{u}_1&\cdots&\sigma_r\vec{u}_r&0&\cdots&0\end{array}\right]}\\
			\onslide<5->{&=\left[\begin{array}{ccc}\vec{u}_1&\cdots&\vec{u}_m\end{array}\right]
			\left[\begin{array}{ccc|c}
			\sigma_1&\cdots&0&\\
			\vdots&\ddots&\vdots&O\\
			0&\cdots&\sigma_r&\\\hline
			&O&&O
			\end{array}\right]}\\
			\onslide<6->{&=U\Sigma}
		\end{align*}
		\begin{itemize}
			\onslide<7->{\item Now, other then exactly defining the ``padding" we have to do with $U$, we have proven Theorem 1.1 on the handout}
			\onslide<8->{\item But how does this help us with image compression?}
		\end{itemize}
	\end{frame}
	
	\begin{frame}{The Outer Product Form of the SVD}
		\begin{itemize}
			\item It turns out we have a different way of representing the SVD
		\end{itemize}
		\pause
		\begin{align*}
			A&=U\Sigma V^{\intercal} \\
			\onslide<5->{&= \left[\begin{array}{ccc}\vec{u}_1&\cdots&\vec{u}_m\end{array}\right]\left[\begin{array}{ccc|c}
			\sigma_1&\cdots&0&\\
			\vdots&\ddots&\vdots&O\\
			0&\cdots&\sigma_r&\\\hline
			&O&&O
			\end{array}\right]\left[\begin{array}{c}\vec{v}_1^{\intercal}\\\vdots\\\vec{v}_n^{\intercal}\end{array}\right]}\\
			\onslide<6->{&= \left[\begin{array}{ccc|ccc}\vec{u}_1&\cdots&\vec{u}_r&\vec{u}_{r+1}&\cdots&\vec{u}_m\end{array}\right]\left[\begin{array}{ccc|c}
			\sigma_1&\cdots&0&\\
			\vdots&\ddots&\vdots&O\\
			0&\cdots&\sigma_r&\\\hline
			&O&&O
			\end{array}\right]\left[\begin{array}{c}\vec{v}_1^{\intercal}\\\vdots\\\vec{v}_r^{\intercal}\\\hline\vec{v}_{r+1}^{\intercal}\\\vdots\\\vec{v}_n^{\intercal}\end{array}\right]\\}
		\end{align*}
	\end{frame}
	
	\begin{frame}{The Outer Product Form of the SVD}
		\begin{align*}
			A &= \left[\begin{array}{ccc}
			\vec{u}_1&\cdots&\vec{u}_r
			\end{array}\right]\left[\begin{array}{ccc}
			\sigma_1&\cdots&0\\
			\vdots&\ddots&\vdots\\
			0&\cdots&\sigma_r
			\end{array}\right]\left[\begin{array}{c}
			\vec{v_1}^{\intercal}\\\vdots\\\vec{v}_r^{\intercal}
			\end{array}\right]\\&\hspace{3cm}+
			\left[\begin{array}{ccc}
			\vec{u}_{r+1}&\cdots&\vec{u}_m
			\end{array}\right]\left[O\right]\left[\begin{array}{c}
			\vec{v_{r+1}}^{\intercal}\\\vdots\\\vec{v}_{n}^{\intercal}
			\end{array}\right]\\			
%			&= \left[\begin{array}{ccc}
%			\vec{u}_1&\cdots&\vec{u}_r
%			\end{array}\right]\left[\begin{array}{ccc}
%			\sigma_1&\cdots&0\\
%			\vdots&\ddots&\vdots\\
%			0&\cdots&\sigma_r
%			\end{array}\right]\left[\begin{array}{c}
%			\vec{v_1}^{\intercal}\\\vdots\\\vec{v}_r^{\intercal}
%			\end{array}\right]+O\\
			\onslide<3->{&= \left[\begin{array}{ccc}
			\sigma_1\vec{u}_1&\cdots&\sigma_r\vec{u}_r
			\end{array}\right]\left[\begin{array}{c}
			\vec{v_1}^{\intercal}\\\vdots\\\vec{v}_r^{\intercal}
			\end{array}\right]\\}
			\onslide<3->{&= \sigma_1\vec{u}_1\vec{v}_1^{\intercal}+\sigma_2\vec{u}_2\vec{v}_2^{\intercal}+\cdots+\sigma_r\vec{u}_r\vec{v}_r^{\intercal}}
		\end{align*}
		\onslide<5->{
		Recall $\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_r>0$.}
	\end{frame}
	
	\begin{frame}{The heart of SVD Compression}
		\[A=\sigma_1\vec{u}_1\vec{v}_1^{\intercal}+\sigma_2\vec{u}_2\vec{v}_2^{\intercal}+\cdots+\sigma_r\vec{u}_r\vec{v}_r^{\intercal}\]
		\pause
		We can talk about an approximation $A_k$ for some $k\le r$.\pause
		\[A_k=\sigma_1\vec{u}_1\vec{v}_1^{\intercal}+\sigma_2\vec{u}_2\vec{v}_2^{\intercal}+\cdots+\sigma_r\vec{u}_k\vec{v}_k^{\intercal}\]
		\[A_r=A\]\pause
		\[\sigma_i\in\mathbb{R},\hspace{0.15cm}\vec{u}_i\in\mathbb{R}^m,\hspace{0.15cm}\vec{v}_i\in\mathbb{R}^n\]
		So for $A_k$ we're really only storing $k(1+n+m)$ numbers.
	\end{frame}
\end{document}